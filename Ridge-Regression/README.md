# ğ—¥ğ—¶ğ—±ğ—´ğ—² ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ˜ƒğ˜€ ğ—Ÿğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»

### ğ—£ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—¥ğ—²ğ—´ğ˜‚ğ—¹ğ—®ğ—¿ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜

---

## ğŸ“Œ ğ—¢ğ˜ƒğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„

This project explores how **Ridge Regression (L2 Regularization)** controls overfitting compared to standard Linear Regression.

Instead of only studying theory, both models were implemented, evaluated, and visualized to observe their behavior on real and synthetic data.

---

## ğŸ¯ ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—²ğ˜€

âœ” Train Linear Regression
âœ” Apply Ridge Regression with multiple alpha values
âœ” Compare performance using RÂ² and RMSE
âœ” Generate high-degree polynomial features to simulate overfitting
âœ” Visualize how regularization smooths predictions

---

## ğŸ“Š ğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜

Diabetes dataset from **scikit-learn**

* 442 samples
* 10 medical features
* Target: disease progression score

---

## âš™ï¸ ğ— ğ—²ğ˜ğ—µğ—¼ğ—±ğ—¼ğ—¹ğ—¼ğ—´ğ˜†

* Linear Regression
* Ridge Regression (L2 penalty)
* Polynomial Features (degree = 16)
* Train/Test split
* Evaluation metrics:

  * RÂ² Score
  * RMSE

---

## ğŸ’» ğ—ğ—²ğ˜† ğ—œğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge

model = Pipeline([
    ('poly', PolynomialFeatures(degree=16)),
    ('ridge', Ridge(alpha=20))
])
```

---

## ğŸ“ˆ ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€ & ğ—¢ğ—¯ğ˜€ğ—²ğ—¿ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€

### Linear vs Ridge

* Linear Regression â†’ unstable with complex features
* Ridge â†’ smoother and more stable predictions

### Effect of Alpha

* Î± = 0 â†’ overfitting
* moderate Î± â†’ best generalization
* large Î± â†’ underfitting
<p align="center">
  <img src="/Plots.pngs/Plot comparing alphas.png" width=100% alt="Banner"/>
</p> 
---

## ğŸ§  ğ—ğ—²ğ˜† ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ğ˜€

* Complex models easily overfit
* Ridge shrinks weights to reduce variance
* Regularization improves generalization
* Same concept is used as **weight decay in deep learning**

---

## ğŸ› ï¸ ğ—§ğ—²ğ—°ğ—µ ğ—¦ğ˜ğ—®ğ—°ğ—¸

* Python
* NumPy
* Matplotlib
* scikit-learn

---

## â–¶ï¸ ğ—›ğ—¼ğ˜„ ğ˜ğ—¼ ğ—¥ğ˜‚ğ—»

```bash
pip install numpy matplotlib scikit-learn
python ridge_experiment.py
```

or open the notebook.

---

## ğŸ‘©â€ğŸ’» ğ—”ğ˜‚ğ˜ğ—µğ—¼ğ—¿

**Isha Maryam**
Undergraduate Computer Science â€¢ Machine Learning â€¢ Deep Learning & AI Enthusiast
